{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcb8e12f-114a-4e16-b184-2e6d3937a49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_signal_from_mat(filepath, key):\n",
    "    try:\n",
    "        mat_data = scipy.io.loadmat(filepath)\n",
    "        signal = mat_data[key].squeeze()\n",
    "        return signal\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {filepath}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "389bf32d-9ad5-420f-99d4-a203d918b7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "\n",
    "def prepare_dataloader(features, labels, test_size=0.2, batch_size=32):\n",
    "    # Normalize features\n",
    "    scaler = StandardScaler()\n",
    "    features = scaler.fit_transform(features)\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features, labels, test_size=test_size, stratify=labels, random_state=42\n",
    "    )\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "    # Create DataLoader\n",
    "    train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "    test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afdab7f7-f189-4f95-87bd-a2f5fd5d600c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.signal import welch\n",
    "from antropy import spectral_entropy, svd_entropy\n",
    "\n",
    "\n",
    "def extract_features(signal, fs=256):\n",
    "    features = []\n",
    "\n",
    "    # --- Time-Domain Features ---\n",
    "    features.append(np.mean(signal))\n",
    "    features.append(np.std(signal))\n",
    "    features.append(np.var(signal))\n",
    "    features.append(skew(signal))\n",
    "    features.append(kurtosis(signal))\n",
    "    features.append(np.max(signal))\n",
    "    features.append(np.min(signal))\n",
    "    features.append(np.median(signal))\n",
    "    features.append(np.percentile(signal, 25))  # Q1\n",
    "    features.append(np.percentile(signal, 75))  # Q3\n",
    "\n",
    "    # --- Frequency-Domain Features ---\n",
    "    freqs, psd = welch(signal, fs)\n",
    "    features.append(np.sum(psd))                         # Total Power\n",
    "    features.append(np.sum(psd[freqs < 4]))              # Delta\n",
    "    features.append(np.sum(psd[(freqs >= 4) & (freqs < 8)]))  # Theta\n",
    "    features.append(np.sum(psd[(freqs >= 8) & (freqs < 12)])) # Alpha\n",
    "    features.append(np.sum(psd[(freqs >= 12) & (freqs < 30)]))# Beta\n",
    "    features.append(np.sum(psd[(freqs >= 30)]))               # Gamma\n",
    "\n",
    "    # --- Entropy Features ---\n",
    "    features.append(spectral_entropy(signal, sf=fs, method='welch'))\n",
    "    features.append(svd_entropy(signal, order=3, delay=1, normalize=True))\n",
    "\n",
    "    return np.array(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d6c2ae0-a30e-419a-b9f4-e2b0c43e42b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import resample\n",
    "\n",
    "def preprocess_signal(signal, fs, dataset):\n",
    "    signal = signal - np.mean(signal)  # Remove DC\n",
    "    if dataset == \"bonn\":\n",
    "        # Already sampled at 173.61 Hz\n",
    "        return signal\n",
    "    elif dataset == \"hauz\":\n",
    "        # Downsample to 250 if needed\n",
    "        if fs != 250:\n",
    "            desired_length = int(len(signal) * 250 / fs)\n",
    "            signal = resample(signal, desired_length)\n",
    "        return signal\n",
    "    else:\n",
    "        return signal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5989a97c-d550-4844-82e1-1878537e9dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_and_extract_features(dataset_path, dataset_name, label_map, fs):\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "\n",
    "    print(f\"\\n--- Loading from: {dataset_path} ---\")\n",
    "\n",
    "    for label_folder, label in label_map.items():\n",
    "        folder_path = os.path.join(dataset_path, label_folder)\n",
    "        if not os.path.exists(folder_path):\n",
    "            print(f\"‚ùå Folder not found: {folder_path}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nüîç Processing folder: {label_folder}\")\n",
    "        for file in tqdm(os.listdir(folder_path), desc=f\"{label_folder} files\"):\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "\n",
    "            if not file.endswith(\".mat\"):\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                mat = loadmat(file_path)\n",
    "                key = label_folder.lower()  # Assume same as folder name\n",
    "                if key not in mat:\n",
    "                    print(f\"‚ö†Ô∏è Key '{key}' not found in {file}\")\n",
    "                    continue\n",
    "\n",
    "                signal = mat[key].squeeze()\n",
    "                if signal.ndim != 1:\n",
    "                    print(f\"‚ö†Ô∏è Non-1D signal in {file}, skipping\")\n",
    "                    continue\n",
    "\n",
    "                # Apply preprocessing\n",
    "                preprocessed = preprocess_signal(signal, fs, dataset=dataset_name)\n",
    "                feats = extract_features(preprocessed, fs)\n",
    "\n",
    "                if feats is None or len(feats) == 0:\n",
    "                    print(f\"‚ö†Ô∏è Empty features from: {file}\")\n",
    "                    continue\n",
    "\n",
    "                all_features.append(feats)\n",
    "                all_labels.append(label)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error processing {file_path}: {e}\")\n",
    "\n",
    "    if not all_features:\n",
    "        raise ValueError(\"‚ùå No features extracted. Please check file keys, preprocessing, and feature functions.\")\n",
    "\n",
    "    print(f\"\\n‚úÖ Loaded {len(all_features)} samples.\")\n",
    "    return np.array(all_features), np.array(all_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ada38c03-0729-4427-a63c-4fd4ff5a35b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def train_model(model, train_loader, optimizer, criterion, device, num_epochs=20):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2a4c139-bd66-404e-96bb-2de819685137",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(y_batch.numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    report = classification_report(all_labels, all_preds)\n",
    "    print(f\"\\nAccuracy: {acc:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(report)\n",
    "    return acc, cm, report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf51c99f-9a7b-4f9d-8ed2-04b7652c703b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TemporalSNNClassifier integrated with your existing EEG pipeline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import snntorch as snn\n",
    "from snntorch import surrogate\n",
    "from snntorch import utils\n",
    "\n",
    "class TemporalSNNClassifier(nn.Module):\n",
    "    def __init__(self, input_features_dim, hidden_neurons, num_classes, T, lif_params):\n",
    "        super(TemporalSNNClassifier, self).__init__()\n",
    "        self.T = T\n",
    "        self.input_features_dim = input_features_dim\n",
    "\n",
    "        beta = lif_params.get(\"beta\", 0.9)\n",
    "        threshold = lif_params.get(\"threshold\", 1.0)\n",
    "        spike_grad = surrogate.fast_sigmoid()\n",
    "\n",
    "        # Layers\n",
    "        self.fc1 = nn.Linear(input_features_dim, hidden_neurons)\n",
    "        self.lif1 = snn.Leaky(beta=beta, threshold=threshold, spike_grad=spike_grad)\n",
    "\n",
    "        self.fc2 = nn.Linear(hidden_neurons, num_classes)\n",
    "        self.lif2 = snn.Leaky(beta=beta, threshold=threshold, spike_grad=spike_grad)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "\n",
    "        spk2_rec = []\n",
    "\n",
    "        for step in range(self.T):\n",
    "            cur_input = x  # You can add noise, jitter, or encode here\n",
    "\n",
    "            cur_input = self.fc1(cur_input)\n",
    "            spk1, mem1 = self.lif1(cur_input, mem1)\n",
    "\n",
    "            cur_input = self.fc2(spk1)\n",
    "            spk2, mem2 = self.lif2(cur_input, mem2)\n",
    "\n",
    "            spk2_rec.append(spk2)\n",
    "\n",
    "        spk2_rec = torch.stack(spk2_rec, dim=0)  # Shape: [T, batch, classes]\n",
    "        out = spk2_rec.sum(dim=0)  # Summing over time\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "# Integration Example:\n",
    "\n",
    "def initialize_model(model_type, input_size, num_classes, model_params, lif_params):\n",
    "    if model_type == 'TemporalSNNClassifier':\n",
    "        model = TemporalSNNClassifier(\n",
    "            input_features_dim=input_size,\n",
    "            hidden_neurons=model_params.get('hidden_neurons', 128),\n",
    "            num_classes=num_classes,\n",
    "            T=model_params.get('simulation_timesteps', 25),\n",
    "            lif_params=lif_params\n",
    "        )\n",
    "        print(f\"Initialized TemporalSNNClassifier with input_size: {input_size}, num_classes: {num_classes}\")\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model type.\")\n",
    "    return model\n",
    "\n",
    "# Usage:\n",
    "# model_params = {\"hidden_neurons\": 128, \"simulation_timesteps\": 25}\n",
    "# lif_params = {\"beta\": 0.9, \"threshold\": 1.0}\n",
    "# model = initialize_model(\"TemporalSNNClassifier\", input_size=features.shape[1], num_classes=2, model_params=model_params, lif_params=lif_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b985d5e-9293-4454-ad0b-fdbc33c62bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SNN_LIF(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SNN_LIF, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.lif1 = nn.LeakyReLU()  # Approximating LIF behavior\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.lif1(x)  # LIF approx\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb1284b8-f4e9-4141-9ec7-3c21bb210dde",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading from: D:\\RESEARCH\\DATABASE\\Neurology_Sleep_Centre_Hauz Khas ---\n",
      "\n",
      "üîç Processing folder: ictal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ictal files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:00<00:00, 171.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Processing folder: interictal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "interictal files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:00<00:00, 185.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Loaded 100 samples.\n",
      "Initialized TemporalSNNClassifier with input_size: 18, num_classes: 2\n",
      "Epoch [1/20], Loss: 0.5608\n",
      "Epoch [2/20], Loss: 0.3129\n",
      "Epoch [3/20], Loss: 0.1846\n",
      "Epoch [4/20], Loss: 0.1440\n",
      "Epoch [5/20], Loss: 0.1049\n",
      "Epoch [6/20], Loss: 0.1109\n",
      "Epoch [7/20], Loss: 0.0791\n",
      "Epoch [8/20], Loss: 0.0915\n",
      "Epoch [9/20], Loss: 0.0687\n",
      "Epoch [10/20], Loss: 0.0765\n",
      "Epoch [11/20], Loss: 0.0679\n",
      "Epoch [12/20], Loss: 0.0506\n",
      "Epoch [13/20], Loss: 0.0609\n",
      "Epoch [14/20], Loss: 0.0387\n",
      "Epoch [15/20], Loss: 0.0258\n",
      "Epoch [16/20], Loss: 0.0365\n",
      "Epoch [17/20], Loss: 0.0417\n",
      "Epoch [18/20], Loss: 0.0342\n",
      "Epoch [19/20], Loss: 0.0214\n",
      "Epoch [20/20], Loss: 0.0150\n",
      "\n",
      "Accuracy: 1.0000\n",
      "Confusion Matrix:\n",
      "[[10  0]\n",
      " [ 0 10]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           1.00        20\n",
      "   macro avg       1.00      1.00      1.00        20\n",
      "weighted avg       1.00      1.00      1.00        20\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.0,\n",
       " array([[10,  0],\n",
       "        [ 0, 10]]),\n",
       " '              precision    recall  f1-score   support\\n\\n           0       1.00      1.00      1.00        10\\n           1       1.00      1.00      1.00        10\\n\\n    accuracy                           1.00        20\\n   macro avg       1.00      1.00      1.00        20\\nweighted avg       1.00      1.00      1.00        20\\n')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load Data\n",
    "features, labels = load_and_extract_features(\n",
    "    dataset_path=r\"D:\\RESEARCH\\DATABASE\\Neurology_Sleep_Centre_Hauz Khas\",\n",
    "    dataset_name=\"hauz\",\n",
    "    label_map={\"ictal\": 1, \"interictal\": 0},\n",
    "    fs=256  # adjust for your dataset\n",
    ")\n",
    "\n",
    "# Prepare loaders\n",
    "train_loader, test_loader = prepare_dataloader(features, labels)\n",
    "\n",
    "# Choose model\n",
    "#model = SNN_LIF(input_size=features.shape[1], hidden_size=128, output_size=2)\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "#model.to(device)\n",
    "\n",
    "model_params = {\"hidden_neurons\": 128, \"simulation_timesteps\": 25}\n",
    "lif_params = {\"beta\": 0.9, \"threshold\": 1.0}\n",
    "model = initialize_model(\"TemporalSNNClassifier\", input_size=features.shape[1], num_classes=2, model_params=model_params, lif_params=lif_params)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# Optimizer and Loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Train\n",
    "train_model(model, train_loader, optimizer, criterion, device)\n",
    "\n",
    "# Evaluate\n",
    "evaluate_model(model, test_loader, device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
